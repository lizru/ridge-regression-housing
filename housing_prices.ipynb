{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e737bce",
   "metadata": {},
   "source": [
    "# Housing Prices\n",
    "This notebook walks through a baseline Ridge Regression model for housing prices prediction. The dataset is from Kaggle, found here: https://www.kaggle.com/datasets/yasserh/housing-prices-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6400f88",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import joblib\n",
    "\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b058a84",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_basic_info(df):\n",
    "    \"\"\"No return, prints basic information & missing values in the given DataFrame.\"\"\"\n",
    "    print(df.head())\n",
    "    print(\"\\nInfo: \")\n",
    "    print(df.info())\n",
    "    print(\"\\nDescription: \")\n",
    "    print(df.describe())\n",
    "    print('\\nMissing values: ')\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1b90d",
   "metadata": {},
   "source": [
    "From the initial summary of the data, we can see that there are 545 rows and no missing values. There are 6 numerical columns, including price, area, bdrooms, bathrooms, stories, and parking. There are 7 categorical columns of object type, consisting of mainroad, guestroom, basement, hotwaterheating, airconditioning, prefarea, and furnishingstatus.\n",
    "\n",
    "The target variable price looks like it may be skewed and has a large standard deviation.\n",
    "\n",
    "Some categorical features like prefarea are similar to binaries and are simply yes/no.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The next step is to visualize some of the data to check for the distribution of the target price, find any outliers, and look at correlations between features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_num_distribution(df, col):\n",
    "    \"\"\"Plots a Histogram/KDE of a given col, designed for numericals.\"\"\"\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.histplot(df[col], bins=30, kde=True)\n",
    "    plt.xlabel(col)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_cat_distribution(df, col):\n",
    "    \"\"\"Plots countplots of a given col, designed for categoricals.\"\"\"\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.countplot(data=df, x=col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_cat_to_target(df, col, target='price'):\n",
    "    \"\"\"Plots boxplot of target grouped by (categorical) col.\"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.boxplot(data=df, x=col, y=target)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"{target} by {col}\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_num_to_target(df, col, target='price'):\n",
    "    \"\"\"Plots scatterplots of (numeric) col vs target.\"\"\"\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.regplot(data=df, x=col, y=target, scatter_kws={'alpha':.5})\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(target)\n",
    "    plt.title(f\"{target} by {col}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def eda_utility(df, cat_cols, num_cols, target='price'):\n",
    "    \"\"\"Loops through all EDA functions above.\"\"\"\n",
    "    plot_num_distribution(df, 'price')\n",
    "    for col in num_cols:\n",
    "        plot_num_distribution(df, col)\n",
    "        plot_num_to_target(df, col)\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        plot_cat_distribution(df, col)\n",
    "        plot_cat_to_target(df, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3c762",
   "metadata": {},
   "source": [
    "From the EDA, we can see several key insights, including:\n",
    "\n",
    "- The target variable `price` is right-skewed, so a log-transformation might improve modeling.  \n",
    "- `Area` is the only truly continuous numerical feature; others like bedrooms and parking are discrete counts better treated as categorical.  \n",
    "- Furnished houses tend to have higher prices.  \n",
    "- Houses located on the main road generally command higher prices.  \n",
    "- More stories, bedrooms, and bathrooms correlate with higher prices.  \n",
    "- Larger area is associated with more expensive houses.  \n",
    "- `Hotwaterheating` is mostly false, limiting its predictive usefulness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f06cc",
   "metadata": {},
   "source": [
    "## Creating model pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fdabe7",
   "metadata": {},
   "source": [
    "We will use Ridge regression for this dataset because:\n",
    "\n",
    "- Some features, like number of bedrooms and bathrooms, are likely correlated, causing multicollinearity issues.\n",
    "\n",
    "- Ridge regression applies L2 regularization, which adds penalties on large coefficients and helps prevent overfitting.\n",
    "\n",
    "- Unlike Lasso, Ridge shrinks coefficients without dropping features entirely, preserving the influence of all variables.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "To create the pipeline, we will make a preprocessor that scales numeric features and uses one hot encoding on categorical features. The data is split into train and test sets, and then the model is fit onto the training set, and evaluated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc33b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessor(num_cols, cat_cols):\n",
    "    \"\"\"\n",
    "    Builds the preprocessing pipelines for numeric and categorical features. \n",
    "    Scales and encodes columns. Returns preprocessing transformer.\n",
    "    \"\"\"\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # One hot encoding is used, does not imply order\n",
    "    categorical_transformer = Pipeline([\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    return ColumnTransformer([\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "        ])\n",
    "\n",
    "\n",
    "def build_model(preprocessor):\n",
    "    \"\"\"Returns the pipeline of preprocessing & the Ridge regression model.\"\"\"\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', Ridge(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, X, y):\n",
    "    \"\"\"Splits data into train and test sets, fits model, and returns the trained pipeline along with test sets.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=.2, random_state=42)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_cv(model, X, y, cv=5):\n",
    "    \"\"\"Evaluates pipeline using cross validation and prints the average RMSE.\"\"\"\n",
    "    cv_scores = cross_val_score(model, X, y, scoring= 'neg_root_mean_squared_error', cv=cv)\n",
    "\n",
    "    # RMSE is a loss metric that scikit-learn returns as negative, so invert scores\n",
    "    rmse_scores = -cv_scores\n",
    "\n",
    "    print(f\"CV RMSE scores: {rmse_scores}\")\n",
    "    print(f\"Average CV RMSE: {rmse_scores.mean():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluates the trained model on the holdout test set.\"\"\"\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "\n",
    "    # Scatterplot of actual vs predicted\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    plt.title('Actual vs Predicted Prices')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Residual plot\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Price')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals vs Predicted Prices')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Residual distribution\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.histplot(residuals, kde=True)\n",
    "    plt.title('Residuals Distribution')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_pipeline(df, target='price'):\n",
    "    \"\"\"\n",
    "    Driver function that runs all modeling steps above, including preprocessing, training, and evaluation.\n",
    "    Returns model.\n",
    "    \"\"\"\n",
    "\n",
    "    # log transform price\n",
    "    df['price'] = np.log(df['price'])\n",
    "    X = df.drop(columns=[target, 'hotwaterheating'])\n",
    "    y = df[target]\n",
    "    \n",
    "\n",
    "    model_num_cols = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "    model_cat_cols = ['mainroad', 'guestroom', 'basement', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "\n",
    "    preprocessor = build_preprocessor(model_num_cols, model_cat_cols)\n",
    "    model = build_model(preprocessor)\n",
    "\n",
    "\n",
    "    # cross validation set\n",
    "    evaluate_model_cv(model, X, y, cv=5)\n",
    "\n",
    "    # holdout set\n",
    "    model, X_test, y_test = train_model(model, X, y)\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e34020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    # load and inspect data\n",
    "    df = pd.read_csv('Housing.csv')\n",
    "    check_basic_info(df)\n",
    "\n",
    "    # define numerical & categorical cols for EDA, excluding target price. Discrete numerical counts are treated as categorical in visualization.\n",
    "    eda_num_cols = ['area']\n",
    "    eda_cat_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', \n",
    "                'prefarea', 'furnishingstatus', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "    \n",
    "    # EDA\n",
    "    eda_utility(df, eda_cat_cols, eda_num_cols, target='price')\n",
    "\n",
    "    # creates & evaluates model\n",
    "    model = run_model_pipeline(df)\n",
    "\n",
    "    # saves model\n",
    "    joblib.dump(model, 'ridge_model.joblib')\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebfe1ff",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "The cross-validation RMSE scores range from about 0.18 to 0.36, showing some variability in how well the model performs depending on the data split. The test RMSE is 0.26. Because these errors are on the log scale, this corresponds roughly to predictions being within Â±29% of the actual house prices on average. The spread in CV scores indicates some sensitivity to the data subsets, likely due to differences in the samples across folds or limited data size.\n",
    "\n",
    "\n",
    "\n",
    "## Steps to Improve Performance\n",
    "\n",
    "- Explore more flexible models like Random Forests or Gradient Boosting to capture complex patterns.  \n",
    "- Perform feature engineering to add interaction terms or polynomial features that may improve predictions.  \n",
    "- Increase dataset size if possible to increase the stability in model performance.  \n",
    "- Experiment with hyperparameter tuning in Ridge regularization strength.  \n",
    "- Investigate and address potential outliers or influential data points that might be affecting performance.  \n",
    "- Continue residual analysis to identify and correct systematic errors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
